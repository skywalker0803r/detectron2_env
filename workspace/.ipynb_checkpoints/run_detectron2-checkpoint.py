import os
import cv2
import numpy as np
import torch
from tqdm import tqdm
import supervision as sv  # ç”¨æ–¼è¿½è¹¤

from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog

# --- è¨­å®šè¼¸å…¥èˆ‡è¼¸å‡ºè³‡æ–™å¤¾ ---
VIDEO_PATH = "data/Yu_Darvish_FF_videos_4S/pitch_0001.mp4"
OUTPUT_DIR = "output_detectron2_first_person_tracked"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- Detectron2 è¨­å®š ---
cfg = get_cfg()
cfg.merge_from_file(
    model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml")
)
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(
    "COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"
)
cfg.MODEL.DEVICE = "cpu"

predictor = DefaultPredictor(cfg)
metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])

# --- è™•ç†å½±ç‰‡ ---
cap = cv2.VideoCapture(VIDEO_PATH)
if not cap.isOpened():
    print(f"âŒ ç„¡æ³•æ‰“é–‹å½±ç‰‡ï¼š{VIDEO_PATH}")
    exit()

frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
fps = cap.get(cv2.CAP_PROP_FPS)
if fps <= 0:
    print("âš ï¸ ç„¡æ³•å–å¾—æ­£ç¢º FPSï¼Œé è¨­ä½¿ç”¨ 30")
    fps = 30

tracker = sv.ByteTrack(frame_rate=int(round(fps)))
target_person_track_id = -1

print(f"è™•ç†å½±ç‰‡ï¼š{VIDEO_PATH}")
print(f"ç¸½å½±æ ¼æ•¸ï¼š{frame_count}")
print(f"å¹€ç‡ (FPS)ï¼š{fps}")

for frame_idx in tqdm(range(frame_count)):
    ret, frame = cap.read()
    if not ret:
        break

    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    outputs = predictor(image_rgb)

    xyxy = outputs["instances"].pred_boxes.tensor.cpu().numpy()
    confidence = outputs["instances"].scores.cpu().numpy()
    class_id = (
        outputs["instances"].pred_classes.cpu().numpy()
        if outputs["instances"].has("pred_classes")
        else np.zeros(len(outputs["instances"]), dtype=int)
    )

    key_points_data = None
    if outputs["instances"].has("pred_keypoints"):
        key_points_data = outputs["instances"].pred_keypoints.cpu().numpy()

    detections = sv.Detections(
        xyxy=xyxy,
        confidence=confidence,
        class_id=class_id,
        data={"key_points": key_points_data} if key_points_data is not None else {},
    )

    detections = tracker.update_with_detections(detections)

    current_target_person_data = None
    if len(detections.tracker_id) > 0:
        if target_person_track_id == -1:
            target_person_track_id = detections.tracker_id[0]
            print(
                f"ğŸ¬ å½±æ ¼ {frame_idx}: é¦–æ¬¡è­˜åˆ¥åˆ°ç›®æ¨™äººç‰©IDç‚º {target_person_track_id}"
            )
            current_target_person_data = {
                "xyxy": detections.xyxy[0],
                "confidence": detections.confidence[0],
                "class_id": detections.class_id[0],
                "key_points": (
                    detections.data["key_points"][0]
                    if "key_points" in detections.data
                    and detections.data["key_points"] is not None
                    else None
                ),
            }
        else:
            target_indices = np.where(detections.tracker_id == target_person_track_id)[
                0
            ]
            if len(target_indices) > 0:
                target_idx = target_indices[0]
                current_target_person_data = {
                    "xyxy": detections.xyxy[target_idx],
                    "confidence": detections.confidence[target_idx],
                    "class_id": detections.class_id[target_idx],
                    "key_points": (
                        detections.data["key_points"][target_idx]
                        if "key_points" in detections.data
                        and detections.data["key_points"] is not None
                        else None
                    ),
                }
            else:
                print(
                    f"ğŸš¨ å½±æ ¼ {frame_idx}: ç›®æ¨™äººç‰©ID {target_person_track_id} ä¸Ÿå¤±ã€‚"
                )

    display_frame = frame.copy()
    if current_target_person_data is not None:
        image_h, image_w, _ = frame.shape
        d2_instances = type(outputs["instances"])((image_h, image_w))

        target_xyxy = current_target_person_data["xyxy"]
        if target_xyxy.ndim == 1:
            target_xyxy = np.expand_dims(target_xyxy, axis=0)
        d2_instances.pred_boxes = outputs["instances"].pred_boxes.__class__(target_xyxy)

        target_confidence = current_target_person_data["confidence"]
        if isinstance(target_confidence, np.ndarray) and target_confidence.ndim == 0:
            score_value = target_confidence.item()
        elif isinstance(target_confidence, (float, np.float32, np.float64)):
            score_value = float(target_confidence)
        else:
            score_value = (
                target_confidence[0].item() if target_confidence.size > 0 else 0.0
            )
        d2_instances.scores = torch.tensor([score_value], dtype=torch.float32)

        if current_target_person_data["key_points"] is not None:
            kp_xy = current_target_person_data["key_points"]
            if kp_xy.ndim == 2:
                kp_xy = np.expand_dims(kp_xy, axis=0)
            if kp_xy.shape[2] == 2:
                kp_conf = np.full((kp_xy.shape[0], kp_xy.shape[1], 1), 2.0)
                kp_xy = np.concatenate((kp_xy, kp_conf), axis=2)
            d2_keypoints_tensor = torch.tensor(kp_xy, dtype=torch.float32)
            d2_instances.pred_keypoints = outputs["instances"].pred_keypoints.__class__(
                d2_keypoints_tensor
            )

        v = Visualizer(
            display_frame[:, :, ::-1],
            metadata=metadata,
            scale=1.0,
            instance_mode=ColorMode.IMAGE,
        )
        vis = v.draw_instance_predictions(d2_instances.to("cpu"))
        display_frame = vis.get_image()[:, :, ::-1]

        if d2_instances.has("pred_keypoints"):
            target_keypoints_to_save = d2_instances.pred_keypoints[0].cpu().numpy()
            np.save(
                os.path.join(OUTPUT_DIR, f"frame_{frame_idx:05d}_target_keypoints.npy"),
                target_keypoints_to_save,
            )

    out_path = os.path.join(OUTPUT_DIR, f"frame_{frame_idx:05d}_annotated.jpg")
    cv2.imwrite(out_path, display_frame)

cap.release()
cv2.destroyAllWindows()
print("å½±ç‰‡è™•ç†å®Œæˆï¼Œæ‰€æœ‰å½±æ ¼åœ–ç‰‡å·²ä¿å­˜ã€‚")
