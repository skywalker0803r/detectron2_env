{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c7ea97a-8fb0-48dc-acef-11d77ade3ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pitch_0043: 100%|██████████| 8/8 [00:00<00:00, 65.84it/s]\n",
      "Loading pitch_0044: 100%|██████████| 238/238 [00:06<00:00, 34.48it/s]\n",
      "Loading pitch_0045: 100%|██████████| 238/238 [00:10<00:00, 22.80it/s]\n",
      "Loading pitch_0046: 100%|██████████| 238/238 [00:05<00:00, 45.82it/s] \n",
      "Loading pitch_0047: 100%|██████████| 136/136 [00:00<00:00, 150.41it/s]\n",
      "Loading pitch_0048: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to features_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "root = 'data'\n",
    "saved_file = 'features_cache.pkl'\n",
    "\n",
    "# 載入已經存的資料\n",
    "if os.path.exists(saved_file):\n",
    "    print(\"Loading saved features...\")\n",
    "    with open(saved_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    features = data['features']\n",
    "    name = data['name']\n",
    "    loaded_names = set(name)\n",
    "else:\n",
    "    features = []\n",
    "    name = []\n",
    "    loaded_names = set()\n",
    "\n",
    "# 讀取新資料夾\n",
    "for subdir_entry in os.scandir(root):\n",
    "    if not subdir_entry.is_dir():\n",
    "        continue\n",
    "    subdir_path = os.path.join(subdir_entry.path, 'KEYPOINT_DIR')\n",
    "    if not os.path.exists(subdir_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        for f_entry in os.scandir(subdir_path):\n",
    "            if not f_entry.is_dir():\n",
    "                continue\n",
    "\n",
    "            if f_entry.path in loaded_names:\n",
    "                continue\n",
    "\n",
    "            feature = []\n",
    "            npy_files = sorted([\n",
    "                entry.path for entry in os.scandir(f_entry.path)\n",
    "                if entry.is_file() and entry.name.endswith('.npy')\n",
    "            ])\n",
    "            for npy_path in tqdm(npy_files, desc=f\"Loading {f_entry.name}\"):\n",
    "                feature.append(np.load(npy_path, allow_pickle=False))\n",
    "            if feature:\n",
    "                features.append(np.array(feature))\n",
    "                name.append(f_entry.path)\n",
    "                loaded_names.add(f_entry.path)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {subdir_entry.name} due to error: {e}\")\n",
    "\n",
    "# 保存到 pickle\n",
    "with open(saved_file, 'wb') as f:\n",
    "    pickle.dump({'features': features, 'name': name}, f)\n",
    "\n",
    "print(f\"Saved features to {saved_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6d88040-5fa3-41b9-ab7f-256d5367fd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 98664.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 17, 3) ball\n",
      "(151, 17, 3) foul\n",
      "(10, 17, 3) hit_into_play\n",
      "(69, 17, 3) hit_into_play\n",
      "(238, 17, 3) ball\n",
      "(98, 17, 3) foul\n",
      "(32, 17, 3) foul\n",
      "(138, 17, 3) called_strike\n",
      "(49, 17, 3) ball\n",
      "(89, 17, 3) ball\n",
      "(120, 17, 3) called_strike\n",
      "(82, 17, 3) ball\n",
      "(105, 17, 3) ball\n",
      "(103, 17, 3) foul\n",
      "(5, 17, 3) ball\n",
      "(238, 17, 3) ball\n",
      "(30, 17, 3) foul\n",
      "(65, 17, 3) ball\n",
      "(238, 17, 3) ball\n",
      "(112, 17, 3) hit_into_play\n",
      "(74, 17, 3) ball\n",
      "(89, 17, 3) blocked_ball\n",
      "(239, 17, 3) foul\n",
      "(17, 17, 3) ball\n",
      "(239, 17, 3) foul\n",
      "(238, 17, 3) ball\n",
      "(238, 17, 3) ball\n",
      "(98, 17, 3) ball\n",
      "(238, 17, 3) blocked_ball\n",
      "(238, 17, 3) ball\n",
      "(236, 17, 3) ball\n",
      "(239, 17, 3) foul\n",
      "(102, 17, 3) hit_into_play\n",
      "(238, 17, 3) ball\n",
      "(119, 17, 3) swinging_strike\n",
      "(52, 17, 3) hit_into_play\n",
      "(9, 17, 3) swinging_strike\n",
      "(118, 17, 3) hit_into_play\n",
      "(98, 17, 3) hit_into_play\n",
      "(238, 17, 3) swinging_strike\n",
      "(238, 17, 3) ball\n",
      "(3, 17, 3) ball\n",
      "(8, 17, 3) ball\n",
      "(238, 17, 3) ball\n",
      "(238, 17, 3) swinging_strike\n",
      "(238, 17, 3) ball\n",
      "(136, 17, 3) ball\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step1: 先找到所有球員 CSV，讀取並合併成一個大 DataFrame\n",
    "csv_cache = {}\n",
    "\n",
    "for j in name:\n",
    "    parts = j.split(os.sep)\n",
    "    player_folder = parts[1]\n",
    "    player_name = player_folder.replace(\"_videos_4S\", \"\")\n",
    "    csv_path = os.path.join('data', player_folder, f\"{player_name}.csv\")\n",
    "    if csv_path not in csv_cache and os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # 建立映射 key 是影片小寫檔名，value 是 description\n",
    "        csv_cache[csv_path] = {k.lower(): v for k, v in zip(df['Filename'], df['description'])}\n",
    "\n",
    "# Step2: 用 cache 查詢 label，速度會快很多\n",
    "y = []\n",
    "for j in tqdm(name):\n",
    "    parts = j.split(os.sep)\n",
    "    player_folder = parts[1]\n",
    "    video_name = parts[-1] + '.mp4'\n",
    "    player_name = player_folder.replace(\"_videos_4S\", \"\")\n",
    "    csv_path = os.path.join('data', player_folder, f\"{player_name}.csv\")\n",
    "\n",
    "    if csv_path in csv_cache:\n",
    "        label = csv_cache[csv_path].get(video_name.lower(), 'Unknown')\n",
    "    else:\n",
    "        label = 'Unknown'\n",
    "\n",
    "    y.append(label)\n",
    "\n",
    "# 測試輸出\n",
    "for i, label in zip(features, y):\n",
    "    print(i.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53982c65-5c64-4161-9738-a93862554843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 建立一個 dict\n",
    "data_dict = {\n",
    "    'feature': features,\n",
    "    'label': y,\n",
    "    'path': name\n",
    "}\n",
    "df = pd.DataFrame(data_dict)\n",
    "df['label_binary'] = df['label'].apply(lambda x: 0 if 'strike' in x.lower() else 1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def pad_or_truncate_sequence(seq, target_len):\n",
    "    \"\"\"統一所有樣本的 seq_len\"\"\"\n",
    "    seq_len, joints, dims = seq.shape\n",
    "    if seq_len == target_len:\n",
    "        return seq\n",
    "    elif seq_len < target_len:\n",
    "        pad_len = target_len - seq_len\n",
    "        pad_shape = (pad_len, joints, dims)\n",
    "        pad = np.zeros(pad_shape, dtype=np.float32)\n",
    "        return np.concatenate([seq, pad], axis=0)\n",
    "    else:\n",
    "        return seq[:target_len]  # truncate\n",
    "target_len = 200  # 例如選定 30 frame 為統一長度\n",
    "df['feature_padded'] = df['feature'].apply(lambda x: pad_or_truncate_sequence(x, target_len))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.stack(df['feature_padded'].values)  # shape: (samples, 30, joints, dims)\n",
    "y = df['label_binary'].values\n",
    "\n",
    "# optional: 合併 joints/dims 維度以餵給 TCN（通常 TCN 接收 (batch, seq_len, input_dim)）\n",
    "X = X.reshape(X.shape[0], X.shape[1], -1)  # → shape: (samples, 30, joints*dims)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5657a3e-cb06-429b-91bc-3529f0a596f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(PoseDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(PoseDataset(X_test, y_test), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b069e93f-d507-4ff2-bac8-6b5df0cf0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.tcn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=2, dilation=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=2, dilation=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, input_dim, seq_len)\n",
    "        x = self.tcn(x)  # (batch, hidden_dim, 1)\n",
    "        x = x.squeeze(-1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70a92e4d-50b6-4e17-8d50-ab48eb9a3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "        preds += torch.argmax(output, dim=1).cpu().tolist()\n",
    "        targets += y_batch.cpu().tolist()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return epoch_loss, acc\n",
    "\n",
    "def eval_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "            preds += torch.argmax(output, dim=1).cpu().tolist()\n",
    "            targets += y_batch.cpu().tolist()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return epoch_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee4f88-f3f3-4909-b53c-4b1c0ed977a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "model = SimpleTCN(input_dim=input_dim).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f145ea-ee49-4808-bfb3-cd585c4c91b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
